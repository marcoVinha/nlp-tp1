{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "date_strftime_format = \"%Y-%m-%y %H:%M:%S\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(asctime)s %(message)s\", datefmt=date_strftime_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "- Source: http://mattmahoney.net/dc/text8.zip\n",
    "- Stored in: `data/train.txt`\n",
    "\n",
    "### Analogies data\n",
    "- Source: https://raw.githubusercontent.com/nicholas-leonard/word2vec/refs/heads/master/questions-words.txt\n",
    "- Stored in: `data/analogies.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def evaluate_analogy(model, word_tuple):\n",
    "    w0_embedding = model.wv[word_tuple[0]]\n",
    "    w1_embedding = model.wv[word_tuple[1]]\n",
    "    w2_embedding = model.wv[word_tuple[2]]\n",
    "    w3_embedding = model.wv[word_tuple[3]]\n",
    "\n",
    "    return distance.cosine(\n",
    "        w1_embedding - w0_embedding,\n",
    "        w3_embedding - w2_embedding,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies_file_name = \"../data/analogies.txt\"\n",
    "\n",
    "with open(analogies_file_name) as file:\n",
    "    file_content = file.read().splitlines()\n",
    "\n",
    "analogies = {}\n",
    "last_key_added = None\n",
    "for line in file_content:\n",
    "    if line[0] == \":\":\n",
    "        last_key_added = line.replace(\": \", \"\")\n",
    "        analogies[last_key_added] = []\n",
    "\n",
    "    else:\n",
    "        analogies[last_key_added].append(\n",
    "            line.lower().split(\" \")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-24 12:02:19 collecting all words and their counts\n",
      "2024-12-24 12:02:20 PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-12-24 12:02:24 collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2024-12-24 12:02:24 Creating a fresh vocabulary\n",
      "2024-12-24 12:02:25 Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 253854 unique words (100.00% of original 253854, drops 0)', 'datetime': '2024-12-01T12:02:25.013508', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2024-12-24 12:02:25 Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 17005207 word corpus (100.00% of original 17005207, drops 0)', 'datetime': '2024-12-01T12:02:25.014478', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2024-12-24 12:02:26 deleting the raw counts dictionary of 253854 items\n",
      "2024-12-24 12:02:26 sample=0.001 downsamples 36 most-common words\n",
      "2024-12-24 12:02:26 Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12819131.785650097 word corpus (75.4%% of prior 17005207)', 'datetime': '2024-12-01T12:02:26.292888', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2024-12-24 12:02:28 estimated required memory for 253854 words and 100 dimensions: 330010200 bytes\n",
      "2024-12-24 12:02:28 resetting layer weights\n",
      "2024-12-24 12:02:28 Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-01T12:02:28.279124', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'build_vocab'}\n",
      "2024-12-24 12:02:28 Word2Vec lifecycle event {'msg': 'training model with 3 workers on 253854 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=9 shrink_windows=True', 'datetime': '2024-12-01T12:02:28.280339', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}\n",
      "Epoch #0 start\n",
      "2024-12-24 12:02:32 EPOCH 0 - PROGRESS: at 0.12% examples, 2456 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:02:35 EPOCH 0 - PROGRESS: at 0.24% examples, 3132 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:02:37 EPOCH 0 - PROGRESS: at 0.35% examples, 3561 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:02:37 EPOCH 0: training on 60000 raw words (30000 effective words) took 8.4s, 3560 effective words/s\n",
      "Epoch #0 end.\n",
      "Training loss: 375898.09375\n",
      "Epoch #1 start\n",
      "2024-12-24 12:02:40 EPOCH 1 - PROGRESS: at 0.12% examples, 3151 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:02:43 EPOCH 1 - PROGRESS: at 0.24% examples, 3093 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:02:45 EPOCH 1 - PROGRESS: at 0.35% examples, 3618 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:02:45 EPOCH 1: training on 60000 raw words (30000 effective words) took 8.3s, 3617 effective words/s\n",
      "Epoch #1 end.\n",
      "Training loss: 679751.5625\n",
      "Epoch #2 start\n",
      "2024-12-24 12:02:49 EPOCH 2 - PROGRESS: at 0.12% examples, 3168 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:02:52 EPOCH 2 - PROGRESS: at 0.24% examples, 3314 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:02:54 EPOCH 2 - PROGRESS: at 0.35% examples, 3711 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:02:54 EPOCH 2: training on 60000 raw words (30000 effective words) took 8.1s, 3711 effective words/s\n",
      "Epoch #2 end.\n",
      "Training loss: 945180.6875\n",
      "Epoch #3 start\n",
      "2024-12-24 12:02:58 EPOCH 3 - PROGRESS: at 0.12% examples, 2867 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:01 EPOCH 3 - PROGRESS: at 0.24% examples, 3293 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:03 EPOCH 3 - PROGRESS: at 0.35% examples, 3556 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:03 EPOCH 3: training on 60000 raw words (30000 effective words) took 8.4s, 3555 effective words/s\n",
      "Epoch #3 end.\n",
      "Training loss: 1183783.125\n",
      "Epoch #4 start\n",
      "2024-12-24 12:03:07 EPOCH 4 - PROGRESS: at 0.12% examples, 3454 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:09 EPOCH 4 - PROGRESS: at 0.24% examples, 3508 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:11 EPOCH 4 - PROGRESS: at 0.35% examples, 3928 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:11 EPOCH 4: training on 60000 raw words (30000 effective words) took 7.6s, 3927 effective words/s\n",
      "Epoch #4 end.\n",
      "Training loss: 1409175.875\n",
      "Epoch #5 start\n",
      "2024-12-24 12:03:16 EPOCH 5 - PROGRESS: at 0.12% examples, 2283 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:19 EPOCH 5 - PROGRESS: at 0.24% examples, 2697 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:21 EPOCH 5 - PROGRESS: at 0.35% examples, 3136 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:21 EPOCH 5: training on 60000 raw words (30000 effective words) took 9.6s, 3136 effective words/s\n",
      "Epoch #5 end.\n",
      "Training loss: 1619244.75\n",
      "Epoch #6 start\n",
      "2024-12-24 12:03:25 EPOCH 6 - PROGRESS: at 0.12% examples, 3179 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:28 EPOCH 6 - PROGRESS: at 0.24% examples, 3383 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:30 EPOCH 6 - PROGRESS: at 0.35% examples, 3664 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:30 EPOCH 6: training on 60000 raw words (30000 effective words) took 8.2s, 3664 effective words/s\n",
      "Epoch #6 end.\n",
      "Training loss: 1820654.625\n",
      "Epoch #7 start\n",
      "2024-12-24 12:03:34 EPOCH 7 - PROGRESS: at 0.12% examples, 3265 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:37 EPOCH 7 - PROGRESS: at 0.24% examples, 3370 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:39 EPOCH 7 - PROGRESS: at 0.35% examples, 3769 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:39 EPOCH 7: training on 60000 raw words (30000 effective words) took 8.0s, 3768 effective words/s\n",
      "Epoch #7 end.\n",
      "Training loss: 2013792.625\n",
      "Epoch #8 start\n",
      "2024-12-24 12:03:43 EPOCH 8 - PROGRESS: at 0.12% examples, 2692 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:46 EPOCH 8 - PROGRESS: at 0.24% examples, 2937 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:48 EPOCH 8 - PROGRESS: at 0.35% examples, 3365 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:48 EPOCH 8: training on 60000 raw words (30000 effective words) took 8.9s, 3364 effective words/s\n",
      "Epoch #8 end.\n",
      "Training loss: 2197128.25\n",
      "Epoch #9 start\n",
      "2024-12-24 12:03:52 EPOCH 9 - PROGRESS: at 0.12% examples, 3034 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:55 EPOCH 9 - PROGRESS: at 0.24% examples, 3299 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:57 EPOCH 9 - PROGRESS: at 0.35% examples, 3712 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:03:57 EPOCH 9: training on 60000 raw words (30000 effective words) took 8.1s, 3711 effective words/s\n",
      "Epoch #9 end.\n",
      "Training loss: 2372521.75\n",
      "Epoch #10 start\n",
      "2024-12-24 12:04:00 EPOCH 10 - PROGRESS: at 0.12% examples, 3090 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:03 EPOCH 10 - PROGRESS: at 0.24% examples, 3281 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:05 EPOCH 10 - PROGRESS: at 0.35% examples, 3711 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:05 EPOCH 10: training on 60000 raw words (30000 effective words) took 8.1s, 3710 effective words/s\n",
      "Epoch #10 end.\n",
      "Training loss: 2542563.75\n",
      "Epoch #11 start\n",
      "2024-12-24 12:04:09 EPOCH 11 - PROGRESS: at 0.12% examples, 3043 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:12 EPOCH 11 - PROGRESS: at 0.24% examples, 3134 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:14 EPOCH 11 - PROGRESS: at 0.35% examples, 3607 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:14 EPOCH 11: training on 60000 raw words (30000 effective words) took 8.3s, 3606 effective words/s\n",
      "Epoch #11 end.\n",
      "Training loss: 2712028.5\n",
      "Epoch #12 start\n",
      "2024-12-24 12:04:18 EPOCH 12 - PROGRESS: at 0.12% examples, 2897 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:21 EPOCH 12 - PROGRESS: at 0.24% examples, 3220 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:23 EPOCH 12 - PROGRESS: at 0.35% examples, 3615 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:23 EPOCH 12: training on 60000 raw words (30000 effective words) took 8.3s, 3614 effective words/s\n",
      "Epoch #12 end.\n",
      "Training loss: 2875918.75\n",
      "Epoch #13 start\n",
      "2024-12-24 12:04:27 EPOCH 13 - PROGRESS: at 0.12% examples, 2979 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:30 EPOCH 13 - PROGRESS: at 0.24% examples, 3168 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:32 EPOCH 13 - PROGRESS: at 0.35% examples, 3593 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:32 EPOCH 13: training on 60000 raw words (30000 effective words) took 8.4s, 3592 effective words/s\n",
      "Epoch #13 end.\n",
      "Training loss: 3036005.75\n",
      "Epoch #14 start\n",
      "2024-12-24 12:04:35 EPOCH 14 - PROGRESS: at 0.12% examples, 3024 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:38 EPOCH 14 - PROGRESS: at 0.24% examples, 3229 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:40 EPOCH 14 - PROGRESS: at 0.35% examples, 3651 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:40 EPOCH 14: training on 60000 raw words (30000 effective words) took 8.2s, 3651 effective words/s\n",
      "Epoch #14 end.\n",
      "Training loss: 3190914.25\n",
      "Epoch #15 start\n",
      "2024-12-24 12:04:45 EPOCH 15 - PROGRESS: at 0.12% examples, 2602 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:47 EPOCH 15 - PROGRESS: at 0.24% examples, 3022 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:50 EPOCH 15 - PROGRESS: at 0.35% examples, 3399 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:50 EPOCH 15: training on 60000 raw words (30000 effective words) took 8.8s, 3399 effective words/s\n",
      "Epoch #15 end.\n",
      "Training loss: 3344994.25\n",
      "Epoch #16 start\n",
      "2024-12-24 12:04:53 EPOCH 16 - PROGRESS: at 0.12% examples, 3039 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:56 EPOCH 16 - PROGRESS: at 0.24% examples, 3261 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:58 EPOCH 16 - PROGRESS: at 0.35% examples, 3637 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:04:58 EPOCH 16: training on 60000 raw words (30000 effective words) took 8.2s, 3637 effective words/s\n",
      "Epoch #16 end.\n",
      "Training loss: 3496180.5\n",
      "Epoch #17 start\n",
      "2024-12-24 12:05:02 EPOCH 17 - PROGRESS: at 0.12% examples, 3343 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:05 EPOCH 17 - PROGRESS: at 0.24% examples, 3305 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:07 EPOCH 17 - PROGRESS: at 0.35% examples, 3710 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:07 EPOCH 17: training on 60000 raw words (30000 effective words) took 8.1s, 3709 effective words/s\n",
      "Epoch #17 end.\n",
      "Training loss: 3644103.0\n",
      "Epoch #18 start\n",
      "2024-12-24 12:05:11 EPOCH 18 - PROGRESS: at 0.12% examples, 2974 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:14 EPOCH 18 - PROGRESS: at 0.24% examples, 3035 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:16 EPOCH 18 - PROGRESS: at 0.35% examples, 3489 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:16 EPOCH 18: training on 60000 raw words (30000 effective words) took 8.6s, 3488 effective words/s\n",
      "Epoch #18 end.\n",
      "Training loss: 3789319.5\n",
      "Epoch #19 start\n",
      "2024-12-24 12:05:20 EPOCH 19 - PROGRESS: at 0.12% examples, 2786 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:23 EPOCH 19 - PROGRESS: at 0.24% examples, 3109 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:25 EPOCH 19 - PROGRESS: at 0.35% examples, 3547 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:25 EPOCH 19: training on 60000 raw words (30000 effective words) took 8.5s, 3546 effective words/s\n",
      "Epoch #19 end.\n",
      "Training loss: 3933112.5\n",
      "Epoch #20 start\n",
      "2024-12-24 12:05:29 EPOCH 20 - PROGRESS: at 0.12% examples, 3018 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:31 EPOCH 20 - PROGRESS: at 0.24% examples, 3210 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:34 EPOCH 20 - PROGRESS: at 0.35% examples, 3614 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:34 EPOCH 20: training on 60000 raw words (30000 effective words) took 8.3s, 3614 effective words/s\n",
      "Epoch #20 end.\n",
      "Training loss: 4074612.5\n",
      "Epoch #21 start\n",
      "2024-12-24 12:05:37 EPOCH 21 - PROGRESS: at 0.12% examples, 3016 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:40 EPOCH 21 - PROGRESS: at 0.24% examples, 3186 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:42 EPOCH 21 - PROGRESS: at 0.35% examples, 3630 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:42 EPOCH 21: training on 60000 raw words (30000 effective words) took 8.3s, 3629 effective words/s\n",
      "Epoch #21 end.\n",
      "Training loss: 4211818.0\n",
      "Epoch #22 start\n",
      "2024-12-24 12:05:46 EPOCH 22 - PROGRESS: at 0.12% examples, 2704 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:50 EPOCH 22 - PROGRESS: at 0.24% examples, 2880 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:52 EPOCH 22 - PROGRESS: at 0.35% examples, 3277 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:52 EPOCH 22: training on 60000 raw words (30000 effective words) took 9.2s, 3277 effective words/s\n",
      "Epoch #22 end.\n",
      "Training loss: 4337016.5\n",
      "Epoch #23 start\n",
      "2024-12-24 12:05:56 EPOCH 23 - PROGRESS: at 0.12% examples, 3136 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:05:58 EPOCH 23 - PROGRESS: at 0.24% examples, 3391 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:00 EPOCH 23 - PROGRESS: at 0.35% examples, 3741 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:00 EPOCH 23: training on 60000 raw words (30000 effective words) took 8.0s, 3740 effective words/s\n",
      "Epoch #23 end.\n",
      "Training loss: 4459446.5\n",
      "Epoch #24 start\n",
      "2024-12-24 12:06:04 EPOCH 24 - PROGRESS: at 0.12% examples, 3213 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:07 EPOCH 24 - PROGRESS: at 0.24% examples, 3302 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:09 EPOCH 24 - PROGRESS: at 0.35% examples, 3616 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:09 EPOCH 24: training on 60000 raw words (30000 effective words) took 8.3s, 3616 effective words/s\n",
      "Epoch #24 end.\n",
      "Training loss: 4580905.0\n",
      "Epoch #25 start\n",
      "2024-12-24 12:06:13 EPOCH 25 - PROGRESS: at 0.12% examples, 3138 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:17 EPOCH 25 - PROGRESS: at 0.24% examples, 2809 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:19 EPOCH 25 - PROGRESS: at 0.35% examples, 3241 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:19 EPOCH 25: training on 60000 raw words (30000 effective words) took 9.3s, 3241 effective words/s\n",
      "Epoch #25 end.\n",
      "Training loss: 4700471.5\n",
      "Epoch #26 start\n",
      "2024-12-24 12:06:23 EPOCH 26 - PROGRESS: at 0.12% examples, 2893 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:26 EPOCH 26 - PROGRESS: at 0.24% examples, 3040 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:28 EPOCH 26 - PROGRESS: at 0.35% examples, 3403 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:28 EPOCH 26: training on 60000 raw words (30000 effective words) took 8.8s, 3402 effective words/s\n",
      "Epoch #26 end.\n",
      "Training loss: 4818491.5\n",
      "Epoch #27 start\n",
      "2024-12-24 12:06:32 EPOCH 27 - PROGRESS: at 0.12% examples, 3233 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:35 EPOCH 27 - PROGRESS: at 0.24% examples, 3352 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:37 EPOCH 27 - PROGRESS: at 0.35% examples, 3685 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:37 EPOCH 27: training on 60000 raw words (30000 effective words) took 8.1s, 3684 effective words/s\n",
      "Epoch #27 end.\n",
      "Training loss: 4934566.0\n",
      "Epoch #28 start\n",
      "2024-12-24 12:06:41 EPOCH 28 - PROGRESS: at 0.12% examples, 3081 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:44 EPOCH 28 - PROGRESS: at 0.24% examples, 3116 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:46 EPOCH 28 - PROGRESS: at 0.35% examples, 3485 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:46 EPOCH 28: training on 60000 raw words (30000 effective words) took 8.6s, 3485 effective words/s\n",
      "Epoch #28 end.\n",
      "Training loss: 5049367.5\n",
      "Epoch #29 start\n",
      "2024-12-24 12:06:50 EPOCH 29 - PROGRESS: at 0.12% examples, 2712 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:53 EPOCH 29 - PROGRESS: at 0.24% examples, 2961 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:56 EPOCH 29 - PROGRESS: at 0.35% examples, 3256 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:06:56 EPOCH 29: training on 60000 raw words (30000 effective words) took 9.2s, 3256 effective words/s\n",
      "Epoch #29 end.\n",
      "Training loss: 5163449.0\n",
      "Epoch #30 start\n",
      "2024-12-24 12:06:59 EPOCH 30 - PROGRESS: at 0.12% examples, 3128 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:02 EPOCH 30 - PROGRESS: at 0.24% examples, 3261 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:04 EPOCH 30 - PROGRESS: at 0.35% examples, 3592 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:04 EPOCH 30: training on 60000 raw words (30000 effective words) took 8.4s, 3592 effective words/s\n",
      "Epoch #30 end.\n",
      "Training loss: 5277770.0\n",
      "Epoch #31 start\n",
      "2024-12-24 12:07:08 EPOCH 31 - PROGRESS: at 0.12% examples, 3233 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:11 EPOCH 31 - PROGRESS: at 0.24% examples, 3295 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:13 EPOCH 31 - PROGRESS: at 0.35% examples, 3608 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:13 EPOCH 31: training on 60000 raw words (30000 effective words) took 8.3s, 3608 effective words/s\n",
      "Epoch #31 end.\n",
      "Training loss: 5388494.0\n",
      "Epoch #32 start\n",
      "2024-12-24 12:07:17 EPOCH 32 - PROGRESS: at 0.12% examples, 3028 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:21 EPOCH 32 - PROGRESS: at 0.24% examples, 2650 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:24 EPOCH 32 - PROGRESS: at 0.35% examples, 3053 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:24 EPOCH 32: training on 60000 raw words (30000 effective words) took 9.8s, 3052 effective words/s\n",
      "Epoch #32 end.\n",
      "Training loss: 5498316.5\n",
      "Epoch #33 start\n",
      "2024-12-24 12:07:28 EPOCH 33 - PROGRESS: at 0.12% examples, 2810 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:31 EPOCH 33 - PROGRESS: at 0.24% examples, 2897 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:33 EPOCH 33 - PROGRESS: at 0.35% examples, 3254 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:33 EPOCH 33: training on 60000 raw words (30000 effective words) took 9.2s, 3254 effective words/s\n",
      "Epoch #33 end.\n",
      "Training loss: 5609235.5\n",
      "Epoch #34 start\n",
      "2024-12-24 12:07:37 EPOCH 34 - PROGRESS: at 0.12% examples, 3307 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:40 EPOCH 34 - PROGRESS: at 0.24% examples, 3365 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:42 EPOCH 34 - PROGRESS: at 0.35% examples, 3696 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:42 EPOCH 34: training on 60000 raw words (30000 effective words) took 8.1s, 3695 effective words/s\n",
      "Epoch #34 end.\n",
      "Training loss: 5719157.5\n",
      "Epoch #35 start\n",
      "2024-12-24 12:07:46 EPOCH 35 - PROGRESS: at 0.12% examples, 3058 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:49 EPOCH 35 - PROGRESS: at 0.24% examples, 3097 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:51 EPOCH 35 - PROGRESS: at 0.35% examples, 3473 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:51 EPOCH 35: training on 60000 raw words (30000 effective words) took 8.6s, 3472 effective words/s\n",
      "Epoch #35 end.\n",
      "Training loss: 5828655.0\n",
      "Epoch #36 start\n",
      "2024-12-24 12:07:55 EPOCH 36 - PROGRESS: at 0.12% examples, 3028 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:07:58 EPOCH 36 - PROGRESS: at 0.24% examples, 3131 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:00 EPOCH 36 - PROGRESS: at 0.35% examples, 3438 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:00 EPOCH 36: training on 60000 raw words (30000 effective words) took 8.7s, 3438 effective words/s\n",
      "Epoch #36 end.\n",
      "Training loss: 5937171.5\n",
      "Epoch #37 start\n",
      "2024-12-24 12:08:04 EPOCH 37 - PROGRESS: at 0.12% examples, 3133 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:07 EPOCH 37 - PROGRESS: at 0.24% examples, 3306 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:09 EPOCH 37 - PROGRESS: at 0.35% examples, 3629 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:09 EPOCH 37: training on 60000 raw words (30000 effective words) took 8.3s, 3628 effective words/s\n",
      "Epoch #37 end.\n",
      "Training loss: 6045393.0\n",
      "Epoch #38 start\n",
      "2024-12-24 12:08:13 EPOCH 38 - PROGRESS: at 0.12% examples, 3156 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:16 EPOCH 38 - PROGRESS: at 0.24% examples, 3194 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:18 EPOCH 38 - PROGRESS: at 0.35% examples, 3552 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:18 EPOCH 38: training on 60000 raw words (30000 effective words) took 8.4s, 3552 effective words/s\n",
      "Epoch #38 end.\n",
      "Training loss: 6152797.0\n",
      "Epoch #39 start\n",
      "2024-12-24 12:08:22 EPOCH 39 - PROGRESS: at 0.12% examples, 3109 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:25 EPOCH 39 - PROGRESS: at 0.24% examples, 2894 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:28 EPOCH 39 - PROGRESS: at 0.35% examples, 3234 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:28 EPOCH 39: training on 60000 raw words (30000 effective words) took 9.3s, 3234 effective words/s\n",
      "Epoch #39 end.\n",
      "Training loss: 6260318.0\n",
      "Epoch #40 start\n",
      "2024-12-24 12:08:32 EPOCH 40 - PROGRESS: at 0.12% examples, 2938 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:35 EPOCH 40 - PROGRESS: at 0.24% examples, 3088 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:37 EPOCH 40 - PROGRESS: at 0.35% examples, 3484 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:37 EPOCH 40: training on 60000 raw words (30000 effective words) took 8.6s, 3483 effective words/s\n",
      "Epoch #40 end.\n",
      "Training loss: 6368594.0\n",
      "Epoch #41 start\n",
      "2024-12-24 12:08:40 EPOCH 41 - PROGRESS: at 0.12% examples, 3201 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:43 EPOCH 41 - PROGRESS: at 0.24% examples, 3367 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:45 EPOCH 41 - PROGRESS: at 0.35% examples, 3783 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:45 EPOCH 41: training on 60000 raw words (30000 effective words) took 7.9s, 3782 effective words/s\n",
      "Epoch #41 end.\n",
      "Training loss: 6476596.5\n",
      "Epoch #42 start\n",
      "2024-12-24 12:08:50 EPOCH 42 - PROGRESS: at 0.12% examples, 2666 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:54 EPOCH 42 - PROGRESS: at 0.24% examples, 2546 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:56 EPOCH 42 - PROGRESS: at 0.35% examples, 2920 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:08:56 EPOCH 42: training on 60000 raw words (30000 effective words) took 10.3s, 2920 effective words/s\n",
      "Epoch #42 end.\n",
      "Training loss: 6582732.0\n",
      "Epoch #43 start\n",
      "2024-12-24 12:09:00 EPOCH 43 - PROGRESS: at 0.12% examples, 2914 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:03 EPOCH 43 - PROGRESS: at 0.24% examples, 3152 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:05 EPOCH 43 - PROGRESS: at 0.35% examples, 3514 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:05 EPOCH 43: training on 60000 raw words (30000 effective words) took 8.5s, 3514 effective words/s\n",
      "Epoch #43 end.\n",
      "Training loss: 6690680.0\n",
      "Epoch #44 start\n",
      "2024-12-24 12:09:09 EPOCH 44 - PROGRESS: at 0.12% examples, 3146 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:12 EPOCH 44 - PROGRESS: at 0.24% examples, 3198 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:14 EPOCH 44 - PROGRESS: at 0.35% examples, 3607 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:14 EPOCH 44: training on 60000 raw words (30000 effective words) took 8.3s, 3606 effective words/s\n",
      "Epoch #44 end.\n",
      "Training loss: 6797079.5\n",
      "Epoch #45 start\n",
      "2024-12-24 12:09:17 EPOCH 45 - PROGRESS: at 0.12% examples, 3287 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:21 EPOCH 45 - PROGRESS: at 0.24% examples, 3229 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:23 EPOCH 45 - PROGRESS: at 0.35% examples, 3608 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:23 EPOCH 45: training on 60000 raw words (30000 effective words) took 8.3s, 3607 effective words/s\n",
      "Epoch #45 end.\n",
      "Training loss: 6904760.0\n",
      "Epoch #46 start\n",
      "2024-12-24 12:09:26 EPOCH 46 - PROGRESS: at 0.12% examples, 3049 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:30 EPOCH 46 - PROGRESS: at 0.24% examples, 2964 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:33 EPOCH 46 - PROGRESS: at 0.35% examples, 3222 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:33 EPOCH 46: training on 60000 raw words (30000 effective words) took 9.3s, 3221 effective words/s\n",
      "Epoch #46 end.\n",
      "Training loss: 7012046.0\n",
      "Epoch #47 start\n",
      "2024-12-24 12:09:36 EPOCH 47 - PROGRESS: at 0.12% examples, 2992 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:39 EPOCH 47 - PROGRESS: at 0.24% examples, 3099 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:41 EPOCH 47 - PROGRESS: at 0.35% examples, 3562 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:41 EPOCH 47: training on 60000 raw words (30000 effective words) took 8.4s, 3562 effective words/s\n",
      "Epoch #47 end.\n",
      "Training loss: 7121607.0\n",
      "Epoch #48 start\n",
      "2024-12-24 12:09:45 EPOCH 48 - PROGRESS: at 0.12% examples, 2998 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:48 EPOCH 48 - PROGRESS: at 0.24% examples, 3083 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:51 EPOCH 48 - PROGRESS: at 0.35% examples, 3386 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:51 EPOCH 48: training on 60000 raw words (30000 effective words) took 8.9s, 3386 effective words/s\n",
      "Epoch #48 end.\n",
      "Training loss: 7230613.5\n",
      "Epoch #49 start\n",
      "2024-12-24 12:09:55 EPOCH 49 - PROGRESS: at 0.12% examples, 3026 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:09:59 EPOCH 49 - PROGRESS: at 0.24% examples, 2675 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:10:01 EPOCH 49 - PROGRESS: at 0.35% examples, 3082 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:10:01 EPOCH 49: training on 60000 raw words (30000 effective words) took 9.7s, 3082 effective words/s\n",
      "Epoch #49 end.\n",
      "Training loss: 7339952.5\n",
      "2024-12-24 12:10:01 Word2Vec lifecycle event {'msg': 'training on 3000000 raw words (1500000 effective words) took 453.3s, 3309 effective words/s', 'datetime': '2024-12-01T12:10:01.590098', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}\n",
      "2024-12-24 12:10:01 Word2Vec lifecycle event {'params': 'Word2Vec<vocab=253854, vector_size=100, alpha=0.025>', 'datetime': '2024-12-01T12:10:01.590910', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f\"Epoch #{self.epoch} start\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\n",
    "            f\"Epoch #{self.epoch} end.\\nTraining loss: {model.get_latest_training_loss()}\"\n",
    "        )\n",
    "        self.epoch += 1\n",
    "\n",
    "word_embedder = Word2Vec(\n",
    "    compute_loss=True,\n",
    "    corpus_file=\"../data/train.txt\",\n",
    "    sg=1,\n",
    "    window=9,\n",
    "    vector_size=100,\n",
    "    epochs=50,\n",
    "    min_count=0,\n",
    "    callbacks=[EpochLogger()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital-common-countries': 0.9048030996349773,\n",
       " 'capital-world': 0.9850809249235741,\n",
       " 'currency': 0.9378736283202299,\n",
       " 'city-in-state': 0.8502208396179038,\n",
       " 'family': 0.9648204714436026,\n",
       " 'gram1-adjective-to-adverb': 0.991789099099487,\n",
       " 'gram2-opposite': 0.8989030008516888,\n",
       " 'gram3-comparative': 0.9528796570307314,\n",
       " 'gram4-superlative': 0.8235985551697462,\n",
       " 'gram5-present-participle': 0.9785777547995275,\n",
       " 'gram6-nationality-adjective': 0.9942383889014595,\n",
       " 'gram7-past-tense': 0.9957298754691126,\n",
       " 'gram8-plural': 0.9741284888124389,\n",
       " 'gram9-plural-verbs': 0.9153059851630441,\n",
       " 'overall_average': 0.9405678406598231}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "report = {k: 0.0 for k in analogies.keys()}\n",
    "for sub_category in report.keys():\n",
    "    report[sub_category] = np.average(\n",
    "        [\n",
    "            evaluate_analogy(word_embedder, curr_sample) for curr_sample in analogies[sub_category]\n",
    "            if all([word_embedder.wv.__contains__(sample) for sample in curr_sample])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "report[\"overall_average\"] = np.average(list(report.values()))\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
