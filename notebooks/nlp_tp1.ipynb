{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "date_strftime_format = \"%Y-%m-%y %H:%M:%S\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(asctime)s %(message)s\", datefmt=date_strftime_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "- Source: http://mattmahoney.net/dc/text8.zip\n",
    "\n",
    "### Analogies data\n",
    "- Source: https://raw.githubusercontent.com/nicholas-leonard/word2vec/refs/heads/master/questions-words.txt\n",
    "- Stored in: `data/analogies.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreLossCurveCallback(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.last_logged_loss = 0\n",
    "        self.loss_curve = []\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        curr_loss = model.get_latest_training_loss() - self.last_logged_loss\n",
    "        self.last_logged_loss = model.get_latest_training_loss()\n",
    "\n",
    "        self.loss_curve.append(curr_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Loss for epoch #{self.epoch}: {curr_loss}\"\n",
    "        )\n",
    "\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        window_size: int,\n",
    "        embedding_size: int,\n",
    "        min_word_count: int = 0,\n",
    "        workers: int = 8,\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self._sg = 1 if model_type ==  \"skipgram\" else 0\n",
    "        self.window_size = window_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.min_word_count = min_word_count\n",
    "        self.workers = workers\n",
    "        self.compute_loss = True\n",
    "\n",
    "        self._loss_container = StoreLossCurveCallback()\n",
    "        self.loss_curve = []\n",
    "\n",
    "        self.model =  None\n",
    "\n",
    "\n",
    "    @property\n",
    "    def wv(self):\n",
    "        return self.model.wv\n",
    "\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        dataset: str,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        self.model = Word2Vec(\n",
    "            sentences=dataset,\n",
    "            sg=self._sg,\n",
    "            window=self.window_size,\n",
    "            vector_size=self.embedding_size,\n",
    "            min_count=self.min_word_count,\n",
    "            epochs=epochs,\n",
    "            compute_loss=self.compute_loss,\n",
    "            callbacks=[self._loss_container],\n",
    "            workers=self.workers,\n",
    "        )\n",
    "\n",
    "        self.loss_curve = self._loss_container.loss_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching best hyper-parameters configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies_file_name = \"../data/analogies.txt\"\n",
    "\n",
    "with open(analogies_file_name) as file:\n",
    "    file_content = file.read().splitlines()\n",
    "\n",
    "all_test_analogies = {}\n",
    "last_key_added = None\n",
    "for line in file_content:\n",
    "    if line[0] == \":\":\n",
    "        last_key_added = line.replace(\": \", \"\")\n",
    "        all_test_analogies[last_key_added] = []\n",
    "\n",
    "    else:\n",
    "        all_test_analogies[last_key_added].append(\n",
    "            line.lower().split(\" \")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_analogy(model, analogy):\n",
    "    def cosine_similarity(a, b):\n",
    "        return (\n",
    "            np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        )\n",
    "\n",
    "    w0_embedding = model.wv[analogy[0]]\n",
    "    w1_embedding = model.wv[analogy[1]]\n",
    "    w2_embedding = model.wv[analogy[2]]\n",
    "    w3_embedding = model.wv[analogy[3]]\n",
    "\n",
    "    return cosine_similarity(\n",
    "        w0_embedding - w1_embedding,\n",
    "        w2_embedding - w3_embedding,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_report(model, test_analogies):\n",
    "    total_ignored_analogies = 0\n",
    "\n",
    "    report = {}\n",
    "    for sub_category in test_analogies.keys():\n",
    "        similarities = []\n",
    "        for curr_sample in test_analogies[sub_category]:\n",
    "            if all([model.wv.__contains__(sample) for sample in curr_sample]):\n",
    "                curr_similarity = evaluate_analogy(model, curr_sample)\n",
    "                similarities.append(curr_similarity)\n",
    "\n",
    "            else:\n",
    "                total_ignored_analogies += 1\n",
    "\n",
    "        report[sub_category] = np.average(similarities)\n",
    "\n",
    "    if total_ignored_analogies:\n",
    "        print(\n",
    "            f\"[WARNING] A total of {total_ignored_analogies} samples were ignored because they contained \"\n",
    "            \"words out of the model's vocabulary.\"\n",
    "        )\n",
    "\n",
    "    report[\"overall_average\"] = np.average(list(report.values()))\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import gensim.downloader as gensim_downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = gensim_downloader.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(\n",
    "    train_dataset,\n",
    "    param_grid: dict,\n",
    "    test_analogies: dict,\n",
    "    param_conditions_callback: callable = None,\n",
    "    return_best: bool = False\n",
    "):\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "    param_keys = list(param_grid.keys())\n",
    "\n",
    "    curr_train = 0\n",
    "\n",
    "    results = []\n",
    "    for params in param_combinations:\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "\n",
    "        if param_conditions_callback and not param_conditions_callback(param_dict):\n",
    "            continue\n",
    "\n",
    "        model = Word2VecModel(\n",
    "            model_type=param_dict[\"model_type\"],\n",
    "            window_size=param_dict[\"window_size\"],\n",
    "            embedding_size=param_dict[\"embedding_size\"],\n",
    "            min_word_count=0,\n",
    "            workers=12,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Starting training model {curr_train}.\"\n",
    "        )\n",
    "\n",
    "        model.train(\n",
    "            dataset=train_dataset,\n",
    "            epochs=param_dict[\"epochs\"],\n",
    "        )\n",
    "\n",
    "        curr_model_report = build_report(model, test_analogies)\n",
    "        score = curr_model_report[\"overall_average\"]\n",
    "\n",
    "        print(\n",
    "            f\"model_type: {model.model_type}, window_size: {model.window_size}, embedding_size: {model.embedding_size}\"\n",
    "        )\n",
    "        print(f\"Final score: {score}\\n\")\n",
    "\n",
    "        results.append(\n",
    "            {\"params\": param_dict, \"score\": score, \"full_report\": curr_model_report, \"model\": model}\n",
    "        )\n",
    "\n",
    "        curr_train += 1\n",
    "\n",
    "    if not return_best:\n",
    "        return results\n",
    "\n",
    "    return max(results, key=lambda x: x[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_param_combination(selected_params: dict):\n",
    "    return (\n",
    "        selected_params[\"window_size\"]\n",
    "        <= selected_params[\"embedding_size\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    \"model_type\": [\"skipgram\"],\n",
    "    \"embedding_size\": [10, 50, 100],\n",
    "    \"window_size\": [5, 15, 25],\n",
    "    \"epochs\": [20],\n",
    "}\n",
    "\n",
    "grid_search_results = run_grid_search(\n",
    "    train_dataset=train_dataset,\n",
    "    param_grid=param_grid,\n",
    "    test_analogies=all_test_analogies,\n",
    "    param_conditions_callback=is_valid_param_combination,\n",
    "    return_best=False\n",
    ")\n",
    "\n",
    "grid_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = max(grid_search_results, key=lambda x: x[\"score\"])\n",
    "best_report = best_model[\"full_report\"]\n",
    "best_score, _ = best_model[\"model\"].wv.evaluate_word_analogies(\"../data/analogies.txt\")\n",
    "\n",
    "\n",
    "plt.plot(best_model[\"model\"].loss_curve)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_model = min(grid_search_results, key=lambda x: x[\"score\"])\n",
    "worst_report = worst_model[\"full_report\"]\n",
    "worst_score, _ = worst_model[\"model\"].wv.evaluate_word_analogies(\"../data/analogies.txt\")\n",
    "\n",
    "plt.plot(worst_model[\"model\"].loss_curve)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
