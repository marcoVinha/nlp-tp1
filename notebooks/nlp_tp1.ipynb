{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "date_strftime_format = \"%Y-%m-%y %H:%M:%S\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(asctime)s %(message)s\", datefmt=date_strftime_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "- Source: http://mattmahoney.net/dc/text8.zip\n",
    "- Stored in: `data/train.txt`\n",
    "\n",
    "### Analogies data\n",
    "- Source: https://raw.githubusercontent.com/nicholas-leonard/word2vec/refs/heads/master/questions-words.txt\n",
    "- Stored in: `data/analogies.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def evaluate_analogy(model, word_tuple):\n",
    "    w0_embedding = model.wv[word_tuple[0]]\n",
    "    w1_embedding = model.wv[word_tuple[1]]\n",
    "    w2_embedding = model.wv[word_tuple[2]]\n",
    "    w3_embedding = model.wv[word_tuple[3]]\n",
    "\n",
    "    return distance.cosine(\n",
    "        w1_embedding - w0_embedding,\n",
    "        w3_embedding - w2_embedding,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['decrease', 'decreases', 'describe', 'describes'],\n",
       " ['decrease', 'decreases', 'eat', 'eats'],\n",
       " ['decrease', 'decreases', 'enhance', 'enhances']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogies_file_name = \"../data/analogies.txt\"\n",
    "\n",
    "with open(analogies_file_name) as file:\n",
    "    file_content = file.read().splitlines()\n",
    "\n",
    "analogies = {}\n",
    "last_key_added = None\n",
    "for line in file_content:\n",
    "    if line[0] == \":\":\n",
    "        last_key_added = line.replace(\": \", \"\")\n",
    "        analogies[last_key_added] = []\n",
    "\n",
    "    else:\n",
    "        analogies[last_key_added].append(\n",
    "            line.lower().split(\" \")\n",
    "        )\n",
    "\n",
    "analogies[last_key_added][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-24 12:14:58 collecting all words and their counts\n",
      "2024-12-24 12:15:00 PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-12-24 12:15:04 collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2024-12-24 12:15:04 Creating a fresh vocabulary\n",
      "2024-12-24 12:15:05 Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 253854 unique words (100.00% of original 253854, drops 0)', 'datetime': '2024-12-01T12:15:05.159232', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2024-12-24 12:15:05 Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 17005207 word corpus (100.00% of original 17005207, drops 0)', 'datetime': '2024-12-01T12:15:05.160197', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2024-12-24 12:15:06 deleting the raw counts dictionary of 253854 items\n",
      "2024-12-24 12:15:06 sample=0.001 downsamples 36 most-common words\n",
      "2024-12-24 12:15:06 Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12819131.785650097 word corpus (75.4%% of prior 17005207)', 'datetime': '2024-12-01T12:15:06.596147', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2024-12-24 12:15:08 estimated required memory for 253854 words and 100 dimensions: 330010200 bytes\n",
      "2024-12-24 12:15:08 resetting layer weights\n",
      "2024-12-24 12:15:08 Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-01T12:15:08.681988', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'build_vocab'}\n",
      "2024-12-24 12:15:08 Word2Vec lifecycle event {'msg': 'training model with 3 workers on 253854 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=9 shrink_windows=True', 'datetime': '2024-12-01T12:15:08.684107', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}\n",
      "Epoch #0 start\n",
      "2024-12-24 12:15:13 EPOCH 0 - PROGRESS: at 0.12% examples, 2454 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:15 EPOCH 0 - PROGRESS: at 0.24% examples, 3123 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:17 EPOCH 0 - PROGRESS: at 0.35% examples, 3429 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:17 EPOCH 0: training on 60000 raw words (30000 effective words) took 8.7s, 3429 effective words/s\n",
      "Epoch #0 end.\n",
      "Training loss: 375898.09375\n",
      "Epoch #1 start\n",
      "2024-12-24 12:15:21 EPOCH 1 - PROGRESS: at 0.12% examples, 3046 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:24 EPOCH 1 - PROGRESS: at 0.24% examples, 3120 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:26 EPOCH 1 - PROGRESS: at 0.35% examples, 3477 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:26 EPOCH 1: training on 60000 raw words (30000 effective words) took 8.6s, 3476 effective words/s\n",
      "Epoch #1 end.\n",
      "Training loss: 683612.25\n",
      "Epoch #2 start\n",
      "2024-12-24 12:15:31 EPOCH 2 - PROGRESS: at 0.12% examples, 2558 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:34 EPOCH 2 - PROGRESS: at 0.24% examples, 2785 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:36 EPOCH 2 - PROGRESS: at 0.35% examples, 3133 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:36 EPOCH 2: training on 60000 raw words (30000 effective words) took 9.6s, 3132 effective words/s\n",
      "Epoch #2 end.\n",
      "Training loss: 957504.6875\n",
      "Epoch #3 start\n",
      "2024-12-24 12:15:43 EPOCH 3 - PROGRESS: at 0.12% examples, 1783 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:46 EPOCH 3 - PROGRESS: at 0.24% examples, 2176 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:50 EPOCH 3 - PROGRESS: at 0.35% examples, 2372 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:50 EPOCH 3: training on 60000 raw words (30000 effective words) took 12.6s, 2372 effective words/s\n",
      "Epoch #3 end.\n",
      "Training loss: 1208767.75\n",
      "Epoch #4 start\n",
      "2024-12-24 12:15:56 EPOCH 4 - PROGRESS: at 0.12% examples, 1925 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:15:59 EPOCH 4 - PROGRESS: at 0.24% examples, 2218 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:02 EPOCH 4 - PROGRESS: at 0.35% examples, 2654 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:02 EPOCH 4: training on 60000 raw words (30000 effective words) took 11.3s, 2654 effective words/s\n",
      "Epoch #4 end.\n",
      "Training loss: 1452132.875\n",
      "Epoch #5 start\n",
      "2024-12-24 12:16:07 EPOCH 5 - PROGRESS: at 0.12% examples, 2108 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:10 EPOCH 5 - PROGRESS: at 0.24% examples, 2596 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:12 EPOCH 5 - PROGRESS: at 0.35% examples, 3053 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:12 EPOCH 5: training on 60000 raw words (30000 effective words) took 9.8s, 3053 effective words/s\n",
      "Epoch #5 end.\n",
      "Training loss: 1684670.875\n",
      "Epoch #6 start\n",
      "2024-12-24 12:16:16 EPOCH 6 - PROGRESS: at 0.12% examples, 2883 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:20 EPOCH 6 - PROGRESS: at 0.24% examples, 3030 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:22 EPOCH 6 - PROGRESS: at 0.35% examples, 3355 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:22 EPOCH 6: training on 60000 raw words (30000 effective words) took 8.9s, 3354 effective words/s\n",
      "Epoch #6 end.\n",
      "Training loss: 1914047.875\n",
      "Epoch #7 start\n",
      "2024-12-24 12:16:27 EPOCH 7 - PROGRESS: at 0.12% examples, 2322 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:31 EPOCH 7 - PROGRESS: at 0.24% examples, 2476 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:33 EPOCH 7 - PROGRESS: at 0.35% examples, 2861 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:33 EPOCH 7: training on 60000 raw words (30000 effective words) took 10.5s, 2860 effective words/s\n",
      "Epoch #7 end.\n",
      "Training loss: 2139486.0\n",
      "Epoch #8 start\n",
      "2024-12-24 12:16:37 EPOCH 8 - PROGRESS: at 0.12% examples, 2579 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:41 EPOCH 8 - PROGRESS: at 0.24% examples, 2544 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:44 EPOCH 8 - PROGRESS: at 0.35% examples, 2953 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:44 EPOCH 8: training on 60000 raw words (30000 effective words) took 10.2s, 2953 effective words/s\n",
      "Epoch #8 end.\n",
      "Training loss: 2361333.25\n",
      "Epoch #9 start\n",
      "2024-12-24 12:16:48 EPOCH 9 - PROGRESS: at 0.12% examples, 2931 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:51 EPOCH 9 - PROGRESS: at 0.24% examples, 3044 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:53 EPOCH 9 - PROGRESS: at 0.35% examples, 3436 words/s, in_qsize -1, out_qsize 1\n",
      "2024-12-24 12:16:53 EPOCH 9: training on 60000 raw words (30000 effective words) took 8.7s, 3435 effective words/s\n",
      "Epoch #9 end.\n",
      "Training loss: 2583688.25\n",
      "2024-12-24 12:16:53 Word2Vec lifecycle event {'msg': 'training on 600000 raw words (300000 effective words) took 105.1s, 2854 effective words/s', 'datetime': '2024-12-01T12:16:53.798519', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}\n",
      "2024-12-24 12:16:53 Word2Vec lifecycle event {'params': 'Word2Vec<vocab=253854, vector_size=100, alpha=0.025>', 'datetime': '2024-12-01T12:16:53.799689', 'gensim': '4.3.3', 'python': '3.10.14 (main, Apr 15 2024, 18:28:39) [Clang 17.0.6 ]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f\"Epoch #{self.epoch} start\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\n",
    "            f\"Epoch #{self.epoch} end.\\nTraining loss: {model.get_latest_training_loss()}\"\n",
    "        )\n",
    "        self.epoch += 1\n",
    "\n",
    "word_embedder = Word2Vec(\n",
    "    compute_loss=True,\n",
    "    corpus_file=\"../data/train.txt\",\n",
    "    sg=1,\n",
    "    window=9,\n",
    "    vector_size=100,\n",
    "    epochs=10,\n",
    "    min_count=0,\n",
    "    callbacks=[EpochLogger()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital-common-countries': 0.8313870096412811,\n",
       " 'capital-world': 0.9775351842601802,\n",
       " 'currency': 0.9472992574106125,\n",
       " 'city-in-state': 0.8127554966517218,\n",
       " 'family': 0.8568015671698033,\n",
       " 'gram1-adjective-to-adverb': 0.9766127064953365,\n",
       " 'gram2-opposite': 0.8496774422774573,\n",
       " 'gram3-comparative': 0.8186714838002181,\n",
       " 'gram4-superlative': 0.6342162429427117,\n",
       " 'gram5-present-participle': 0.9596752007156611,\n",
       " 'gram6-nationality-adjective': 0.9999506674276156,\n",
       " 'gram7-past-tense': 0.9879978406432277,\n",
       " 'gram8-plural': 0.9253952315140421,\n",
       " 'gram9-plural-verbs': 0.815993753776183,\n",
       " 'overall_average': 0.8852835060518609}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "report = {k: 0.0 for k in analogies.keys()}\n",
    "for sub_category in report.keys():\n",
    "    report[sub_category] = np.average(\n",
    "        [\n",
    "            evaluate_analogy(word_embedder, curr_sample) for curr_sample in analogies[sub_category]\n",
    "            if all([word_embedder.wv.__contains__(sample) for sample in curr_sample])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "report[\"overall_average\"] = np.average(list(report.values()))\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
