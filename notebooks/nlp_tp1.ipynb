{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "date_strftime_format = \"%Y-%m-%y %H:%M:%S\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING, format=\"%(asctime)s %(message)s\", datefmt=date_strftime_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "- Source: http://mattmahoney.net/dc/text8.zip\n",
    "- Stored in: `data/train.txt`\n",
    "\n",
    "### Analogies data\n",
    "- Source: https://raw.githubusercontent.com/nicholas-leonard/word2vec/refs/heads/master/questions-words.txt\n",
    "- Stored in: `data/analogies.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.word2vec import Text8Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreLossCurveCallback(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.last_logged_loss = 0\n",
    "        self.loss_curve = []\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        curr_loss = model.get_latest_training_loss() - self.last_logged_loss\n",
    "        self.last_logged_loss = model.get_latest_training_loss()\n",
    "\n",
    "        self.loss_curve.append(curr_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Loss for epoch #{self.epoch}: {curr_loss}\"\n",
    "        )\n",
    "\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        window_size: int,\n",
    "        embedding_size: int,\n",
    "        min_word_count: int = 0,\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self._sg = 1 if model_type ==  \"skipgram\" else 0\n",
    "        self.window_size = window_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.min_word_count = min_word_count\n",
    "        self.compute_loss = True\n",
    "\n",
    "        self._loss_container = StoreLossCurveCallback()\n",
    "        self.loss_curve = []\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "\n",
    "    @property\n",
    "    def wv(self):\n",
    "        return self.model.wv\n",
    "\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        training_corpus_fpath: str,\n",
    "        epochs: int,\n",
    "        workers: int = 8,\n",
    "    ):\n",
    "        self.model = Word2Vec(\n",
    "            sentences=Text8Corpus(fname=training_corpus_fpath),\n",
    "            sg=self._sg,\n",
    "            window=self.window_size,\n",
    "            vector_size=self.embedding_size,\n",
    "            epochs=epochs,\n",
    "            min_count=self.min_word_count,\n",
    "            compute_loss=self.compute_loss,\n",
    "            callbacks=[self._loss_container],\n",
    "            workers=workers,\n",
    "        )\n",
    "\n",
    "        self.loss_curve = self._loss_container.loss_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching best hyper-parameters configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies_file_name = \"../data/analogies.txt\"\n",
    "\n",
    "with open(analogies_file_name) as file:\n",
    "    file_content = file.read().splitlines()\n",
    "\n",
    "all_test_analogies = {}\n",
    "last_key_added = None\n",
    "for line in file_content:\n",
    "    if line[0] == \":\":\n",
    "        last_key_added = line.replace(\": \", \"\")\n",
    "        all_test_analogies[last_key_added] = []\n",
    "\n",
    "    else:\n",
    "        all_test_analogies[last_key_added].append(\n",
    "            line.lower().split(\" \")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_analogy(model, word_tuple):\n",
    "    w0_embedding = model.wv[word_tuple[0]]\n",
    "    w1_embedding = model.wv[word_tuple[1]]\n",
    "    w2_embedding = model.wv[word_tuple[2]]\n",
    "    w3_embedding = model.wv[word_tuple[3]]\n",
    "\n",
    "    return 1 - distance.cosine(\n",
    "        w1_embedding - w0_embedding,\n",
    "        w3_embedding - w2_embedding,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_report(model, test_analogies):\n",
    "    total_ignored_analogies = 0\n",
    "\n",
    "    report = {k: 0.0 for k in test_analogies.keys()}\n",
    "    for sub_category in report.keys():\n",
    "        similarities = []\n",
    "        for curr_sample in test_analogies[sub_category]:\n",
    "            if all([model.wv.__contains__(sample) for sample in curr_sample]):\n",
    "                curr_similarity = evaluate_analogy(model, curr_sample)\n",
    "                similarities.append(curr_similarity)\n",
    "\n",
    "            else:\n",
    "                total_ignored_analogies += 1\n",
    "        \n",
    "        report[sub_category] = np.average(similarities)\n",
    "\n",
    "    if total_ignored_analogies:\n",
    "        print(\n",
    "            f\"[WARNING] A total of {total_ignored_analogies} samples were ignored because they contained \"\n",
    "            \"words out of the model's vocabulary.\"\n",
    "        )\n",
    "\n",
    "    report[\"overall_average\"] = np.average(list(report.values()))\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(\n",
    "    param_grid: dict,\n",
    "    test_analogies: dict,\n",
    "    param_conditions_callback: callable = None,\n",
    "    return_best: bool = False\n",
    "):\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "    param_keys = list(param_grid.keys())\n",
    "\n",
    "    curr_train = 0\n",
    "\n",
    "    results = []\n",
    "    for params in param_combinations:\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "\n",
    "        if not param_conditions_callback(param_dict):\n",
    "            continue\n",
    "\n",
    "        model = Word2VecModel(\n",
    "            model_type=param_dict[\"model_type\"],\n",
    "            window_size=param_dict[\"window_size\"],\n",
    "            embedding_size=param_dict[\"embedding_size\"],\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Starting training model {curr_train + 1} of {len(param_combinations)}\"\n",
    "        )\n",
    "\n",
    "        model.train(\n",
    "            training_corpus_fpath=\"../data/train.txt\",\n",
    "            epochs=param_dict[\"epochs\"],\n",
    "        )\n",
    "\n",
    "        curr_model_report = build_report(model, test_analogies)\n",
    "        score = curr_model_report[\"overall_average\"]\n",
    "\n",
    "        print(\n",
    "            f\"model_type: {model.model_type}, window_size: {model.window_size}, embedding_size: {model.embedding_size}\"\n",
    "        )\n",
    "        print(f\"Final score: {score}\\n\")\n",
    "\n",
    "        results.append(\n",
    "            {\"params\": param_dict, \"score\": score, \"full_report\": curr_model_report}\n",
    "        )\n",
    "\n",
    "        curr_train += 1\n",
    "\n",
    "    if not return_best:\n",
    "        return results\n",
    "\n",
    "    return max(results, key=lambda x: x[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_param_combination(selected_params: dict):\n",
    "    return (\n",
    "        selected_params[\"window_size\"]\n",
    "        <= selected_params[\"epochs\"]\n",
    "        <= selected_params[\"embedding_size\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training model 1 of 1\n",
      "Loss for epoch #0: 3925150.75\n",
      "Loss for epoch #1: 3213225.25\n",
      "Loss for epoch #2: 2910120.0\n",
      "Loss for epoch #3: 2694924.0\n",
      "Loss for epoch #4: 2703890.0\n",
      "[WARNING] A total of 438 were ignored because they contained words out of the model's vocabulary.\n",
      "model_type: cbow, window_size: 3, embedding_size: 5\n",
      "Final score: 0.42961090919634254\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'model_type': 'cbow',\n",
       "  'embedding_size': 5,\n",
       "  'window_size': 3,\n",
       "  'epochs': 5},\n",
       " 'score': 0.42961090919634254,\n",
       " 'full_report': {'capital-common-countries': 0.686720417587637,\n",
       "  'capital-world': 0.7839351314017419,\n",
       "  'currency': 0.7610609295174078,\n",
       "  'city-in-state': 0.7584993818520714,\n",
       "  'family': 0.0928185977840391,\n",
       "  'gram1-adjective-to-adverb': 0.1774767649038595,\n",
       "  'gram2-opposite': 0.34514809483209075,\n",
       "  'gram3-comparative': 0.01765687989036821,\n",
       "  'gram4-superlative': 0.44256560337404843,\n",
       "  'gram5-present-participle': 0.3286936293923346,\n",
       "  'gram6-nationality-adjective': 0.6962941432764763,\n",
       "  'gram7-past-tense': 0.2437719318179426,\n",
       "  'gram8-plural': 0.2817761093764548,\n",
       "  'gram9-plural-verbs': 0.3981351137423251,\n",
       "  'overall_average': 0.42961090919634254}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"model_type\": [\"cbow\"],\n",
    "    \"embedding_size\": [5],\n",
    "    \"window_size\": [3],\n",
    "    \"epochs\": [5],\n",
    "}\n",
    "\n",
    "grid_search_results = run_grid_search(\n",
    "    param_grid=param_grid,\n",
    "    test_analogies=all_test_analogies,\n",
    "    param_conditions_callback=is_valid_param_combination,\n",
    "    return_best=True\n",
    ")\n",
    "\n",
    "grid_search_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
