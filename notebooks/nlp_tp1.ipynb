{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "date_strftime_format = \"%Y-%m-%y %H:%M:%S\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING, format=\"%(asctime)s %(message)s\", datefmt=date_strftime_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "- Source: http://mattmahoney.net/dc/text8.zip\n",
    "\n",
    "### Analogies data\n",
    "- Source: https://raw.githubusercontent.com/nicholas-leonard/word2vec/refs/heads/master/questions-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreLossCurveCallback(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.last_logged_loss = 0\n",
    "        self.loss_curve = []\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        curr_loss = model.get_latest_training_loss() - self.last_logged_loss\n",
    "        self.last_logged_loss = model.get_latest_training_loss()\n",
    "\n",
    "        self.loss_curve.append(curr_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Loss for epoch #{self.epoch}: {curr_loss}\"\n",
    "        )\n",
    "\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        window_size: int,\n",
    "        embedding_size: int,\n",
    "        min_word_count: int = 0,\n",
    "        workers: int = 8,\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self._sg = 1 if model_type ==  \"skipgram\" else 0\n",
    "        self.window_size = window_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.min_word_count = min_word_count\n",
    "        self.workers = workers\n",
    "        self.compute_loss = True\n",
    "\n",
    "        self._loss_container = StoreLossCurveCallback()\n",
    "        self.loss_curve = []\n",
    "\n",
    "        self.model =  None\n",
    "\n",
    "\n",
    "    @property\n",
    "    def wv(self):\n",
    "        return self.model.wv\n",
    "\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        dataset: str,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        self.model = Word2Vec(\n",
    "            sentences=dataset,\n",
    "            sg=self._sg,\n",
    "            window=self.window_size,\n",
    "            vector_size=self.embedding_size,\n",
    "            min_count=self.min_word_count,\n",
    "            epochs=epochs,\n",
    "            compute_loss=self.compute_loss,\n",
    "            callbacks=[self._loss_container],\n",
    "            workers=self.workers,\n",
    "        )\n",
    "\n",
    "        self.loss_curve = self._loss_container.loss_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching best hyper-parameters configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies_file_name = \"../data/analogies.txt\"\n",
    "\n",
    "with open(analogies_file_name) as file:\n",
    "    file_content = file.read().splitlines()\n",
    "\n",
    "all_test_analogies = {}\n",
    "last_key_added = None\n",
    "for line in file_content:\n",
    "    if line[0] == \":\":\n",
    "        last_key_added = line.replace(\": \", \"\")\n",
    "        all_test_analogies[last_key_added] = []\n",
    "\n",
    "    else:\n",
    "        all_test_analogies[last_key_added].append(\n",
    "            line.lower().split(\" \")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_analogy(model, analogy):\n",
    "    def cosine_similarity(a, b):\n",
    "        return (\n",
    "            np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        )\n",
    "\n",
    "    w0_embedding = model.wv[analogy[0]]\n",
    "    w1_embedding = model.wv[analogy[1]]\n",
    "    w2_embedding = model.wv[analogy[2]]\n",
    "    w3_embedding = model.wv[analogy[3]]\n",
    "\n",
    "    return cosine_similarity(\n",
    "        w0_embedding - w1_embedding + w3_embedding,\n",
    "        w2_embedding,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_report(model, test_analogies):\n",
    "    total_ignored_analogies = 0\n",
    "\n",
    "    report = {}\n",
    "    for sub_category in test_analogies.keys():\n",
    "        similarities = []\n",
    "        for curr_sample in test_analogies[sub_category]:\n",
    "            if all([model.wv.__contains__(sample) for sample in curr_sample]):\n",
    "                curr_similarity = evaluate_analogy(model, curr_sample)\n",
    "                similarities.append(curr_similarity)\n",
    "\n",
    "            else:\n",
    "                total_ignored_analogies += 1\n",
    "\n",
    "        report[sub_category] = np.average(similarities)\n",
    "\n",
    "    if total_ignored_analogies:\n",
    "        print(\n",
    "            f\"[WARNING] A total of {total_ignored_analogies} samples were ignored because they contained \"\n",
    "            \"words out of the model's vocabulary.\"\n",
    "        )\n",
    "\n",
    "    report[\"overall_average\"] = np.average(list(report.values()))\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import gensim.downloader as gensim_downloader\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = gensim_downloader.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(\n",
    "    train_dataset,\n",
    "    param_grid: dict,\n",
    "    test_analogies: dict,\n",
    "    return_best: bool = False\n",
    "):\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "    param_keys = list(param_grid.keys())\n",
    "\n",
    "    curr_train = 0\n",
    "\n",
    "    results = []\n",
    "    for params in param_combinations:\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "\n",
    "        model = Word2VecModel(\n",
    "            model_type=param_dict[\"model_type\"],\n",
    "            window_size=param_dict[\"window_size\"],\n",
    "            embedding_size=param_dict[\"embedding_size\"],\n",
    "            min_word_count=0,\n",
    "            workers=12,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Starting training model {curr_train}. Will train for {param_dict['epochs']} epochs.\"\n",
    "        )\n",
    "        print(\n",
    "            f\"model_type: {model.model_type}, window_size: {model.window_size}, embedding_size: {model.embedding_size}\"\n",
    "        )\n",
    "\n",
    "        model.train(\n",
    "            dataset=train_dataset,\n",
    "            epochs=param_dict[\"epochs\"],\n",
    "        )\n",
    "\n",
    "        curr_model_report = build_report(model, test_analogies)\n",
    "        score = curr_model_report[\"overall_average\"]\n",
    "\n",
    "        print(f\"Final analogies score: {score}\\n\")\n",
    "\n",
    "        results.append(\n",
    "            {\"params\": param_dict, \"score\": score, \"full_report\": curr_model_report}\n",
    "        )\n",
    "\n",
    "        curr_train += 1\n",
    "\n",
    "    if not return_best:\n",
    "        return results\n",
    "\n",
    "    return max(results, key=lambda x: x[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training model 0. Will train for 2 epochs.\n",
      "model_type: skipgram, window_size: 3, embedding_size: 5\n",
      "Loss for epoch #0: 14831020.0\n",
      "Loss for epoch #1: 11385590.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.911324679851532\n",
      "\n",
      "Starting training model 1. Will train for 5 epochs.\n",
      "model_type: skipgram, window_size: 3, embedding_size: 5\n",
      "Loss for epoch #0: 14525665.0\n",
      "Loss for epoch #1: 10843795.0\n",
      "Loss for epoch #2: 9537772.0\n",
      "Loss for epoch #3: 5853708.0\n",
      "Loss for epoch #4: 5772788.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.9100650548934937\n",
      "\n",
      "Starting training model 2. Will train for 10 epochs.\n",
      "model_type: skipgram, window_size: 3, embedding_size: 5\n",
      "Loss for epoch #0: 14938645.0\n",
      "Loss for epoch #1: 10696035.0\n",
      "Loss for epoch #2: 9501100.0\n",
      "Loss for epoch #3: 6105528.0\n",
      "Loss for epoch #4: 5940780.0\n",
      "Loss for epoch #5: 5967468.0\n",
      "Loss for epoch #6: 5817036.0\n",
      "Loss for epoch #7: 5854748.0\n",
      "Loss for epoch #8: 2441372.0\n",
      "Loss for epoch #9: 237776.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.9032546281814575\n",
      "\n",
      "Starting training model 3. Will train for 2 epochs.\n",
      "model_type: skipgram, window_size: 7, embedding_size: 5\n",
      "Loss for epoch #0: 16571354.0\n",
      "Loss for epoch #1: 13238190.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.9123977422714233\n",
      "\n",
      "Starting training model 4. Will train for 5 epochs.\n",
      "model_type: skipgram, window_size: 7, embedding_size: 5\n",
      "Loss for epoch #0: 15888228.0\n",
      "Loss for epoch #1: 12047904.0\n",
      "Loss for epoch #2: 9533884.0\n",
      "Loss for epoch #3: 6941348.0\n",
      "Loss for epoch #4: 6677348.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.9034348726272583\n",
      "\n",
      "Starting training model 5. Will train for 10 epochs.\n",
      "model_type: skipgram, window_size: 7, embedding_size: 5\n",
      "Loss for epoch #0: 16760597.0\n",
      "Loss for epoch #1: 11730859.0\n",
      "Loss for epoch #2: 9318116.0\n",
      "Loss for epoch #3: 6776108.0\n",
      "Loss for epoch #4: 6922052.0\n",
      "Loss for epoch #5: 7151644.0\n",
      "Loss for epoch #6: 6981264.0\n",
      "Loss for epoch #7: 1697016.0\n",
      "Loss for epoch #8: 256288.0\n",
      "Loss for epoch #9: 243880.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.8928728103637695\n",
      "\n",
      "Starting training model 6. Will train for 2 epochs.\n",
      "model_type: skipgram, window_size: 15, embedding_size: 5\n",
      "Loss for epoch #0: 25834384.0\n",
      "Loss for epoch #1: 17707348.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.9070865511894226\n",
      "\n",
      "Starting training model 7. Will train for 5 epochs.\n",
      "model_type: skipgram, window_size: 15, embedding_size: 5\n",
      "Loss for epoch #0: 25869060.0\n",
      "Loss for epoch #1: 16745536.0\n",
      "Loss for epoch #2: 13568264.0\n",
      "Loss for epoch #3: 11052012.0\n",
      "Loss for epoch #4: 504544.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.8920749425888062\n",
      "\n",
      "Starting training model 8. Will train for 10 epochs.\n",
      "model_type: skipgram, window_size: 15, embedding_size: 5\n",
      "Loss for epoch #0: 25360148.0\n",
      "Loss for epoch #1: 16749840.0\n",
      "Loss for epoch #2: 13527348.0\n",
      "Loss for epoch #3: 11593744.0\n",
      "Loss for epoch #4: 781496.0\n",
      "Loss for epoch #5: 752184.0\n",
      "Loss for epoch #6: 671440.0\n",
      "Loss for epoch #7: 648696.0\n",
      "Loss for epoch #8: 567816.0\n",
      "Loss for epoch #9: 515144.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.8823021054267883\n",
      "\n",
      "Starting training model 9. Will train for 2 epochs.\n",
      "model_type: skipgram, window_size: 3, embedding_size: 25\n",
      "Loss for epoch #0: 9011655.0\n",
      "Loss for epoch #1: 7388721.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.806411623954773\n",
      "\n",
      "Starting training model 10. Will train for 5 epochs.\n",
      "model_type: skipgram, window_size: 3, embedding_size: 25\n",
      "Loss for epoch #0: 9224594.0\n",
      "Loss for epoch #1: 7301599.0\n",
      "Loss for epoch #2: 5881269.0\n",
      "Loss for epoch #3: 5861158.0\n",
      "Loss for epoch #4: 5696472.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.7852297425270081\n",
      "\n",
      "Starting training model 11. Will train for 10 epochs.\n",
      "model_type: skipgram, window_size: 3, embedding_size: 25\n",
      "Loss for epoch #0: 8926796.0\n",
      "Loss for epoch #1: 7486820.0\n",
      "Loss for epoch #2: 5926444.0\n",
      "Loss for epoch #3: 5829174.0\n",
      "Loss for epoch #4: 5649698.0\n",
      "Loss for epoch #5: 3405652.0\n",
      "Loss for epoch #6: 3372484.0\n",
      "Loss for epoch #7: 3179980.0\n",
      "Loss for epoch #8: 3225064.0\n",
      "Loss for epoch #9: 3263168.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.76535564661026\n",
      "\n",
      "Starting training model 12. Will train for 2 epochs.\n",
      "model_type: skipgram, window_size: 7, embedding_size: 25\n",
      "Loss for epoch #0: 16002858.0\n",
      "Loss for epoch #1: 12533948.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.8024758696556091\n",
      "\n",
      "Starting training model 13. Will train for 5 epochs.\n",
      "model_type: skipgram, window_size: 7, embedding_size: 25\n",
      "Loss for epoch #0: 15937065.0\n",
      "Loss for epoch #1: 11375081.0\n",
      "Loss for epoch #2: 9308346.0\n",
      "Loss for epoch #3: 6827376.0\n",
      "Loss for epoch #4: 6402896.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.7686783075332642\n",
      "\n",
      "Starting training model 14. Will train for 10 epochs.\n",
      "model_type: skipgram, window_size: 7, embedding_size: 25\n",
      "Loss for epoch #0: 16273986.0\n",
      "Loss for epoch #1: 11644244.0\n",
      "Loss for epoch #2: 9050582.0\n",
      "Loss for epoch #3: 6567864.0\n",
      "Loss for epoch #4: 6619784.0\n",
      "Loss for epoch #5: 6645088.0\n",
      "Loss for epoch #6: 6359468.0\n",
      "Loss for epoch #7: 4076560.0\n",
      "Loss for epoch #8: 335352.0\n",
      "Loss for epoch #9: 323880.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.7431932091712952\n",
      "\n",
      "Starting training model 15. Will train for 2 epochs.\n",
      "model_type: skipgram, window_size: 15, embedding_size: 25\n",
      "Loss for epoch #0: 26299392.0\n",
      "Loss for epoch #1: 16713800.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.7836975455284119\n",
      "\n",
      "Starting training model 16. Will train for 5 epochs.\n",
      "model_type: skipgram, window_size: 15, embedding_size: 25\n",
      "Loss for epoch #0: 25978474.0\n",
      "Loss for epoch #1: 15748802.0\n",
      "Loss for epoch #2: 13032232.0\n",
      "Loss for epoch #3: 12373828.0\n",
      "Loss for epoch #4: 608008.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.75188809633255\n",
      "\n",
      "Starting training model 17. Will train for 10 epochs.\n",
      "model_type: skipgram, window_size: 15, embedding_size: 25\n",
      "Loss for epoch #0: 25465002.0\n",
      "Loss for epoch #1: 15692914.0\n",
      "Loss for epoch #2: 12623028.0\n",
      "Loss for epoch #3: 12754068.0\n",
      "Loss for epoch #4: 1476028.0\n",
      "Loss for epoch #5: 896896.0\n",
      "Loss for epoch #6: 845080.0\n",
      "Loss for epoch #7: 781032.0\n",
      "Loss for epoch #8: 738320.0\n",
      "Loss for epoch #9: 657872.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.7327658534049988\n",
      "\n",
      "Starting training model 18. Will train for 2 epochs.\n",
      "model_type: skipgram, window_size: 3, embedding_size: 100\n",
      "Loss for epoch #0: 9435025.0\n",
      "Loss for epoch #1: 7440935.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.6996349692344666\n",
      "\n",
      "Starting training model 19. Will train for 5 epochs.\n",
      "model_type: skipgram, window_size: 3, embedding_size: 100\n",
      "Loss for epoch #0: 9139200.0\n",
      "Loss for epoch #1: 7124806.0\n",
      "Loss for epoch #2: 6286364.0\n",
      "Loss for epoch #3: 6820010.0\n",
      "Loss for epoch #4: 5350112.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.6279444098472595\n",
      "\n",
      "Starting training model 20. Will train for 10 epochs.\n",
      "model_type: skipgram, window_size: 3, embedding_size: 100\n",
      "Loss for epoch #0: 9166018.0\n",
      "Loss for epoch #1: 7718614.0\n",
      "Loss for epoch #2: 6360400.0\n",
      "Loss for epoch #3: 5924766.0\n",
      "Loss for epoch #4: 5575206.0\n",
      "Loss for epoch #5: 3531804.0\n",
      "Loss for epoch #6: 3341468.0\n",
      "Loss for epoch #7: 3495192.0\n",
      "Loss for epoch #8: 3249900.0\n",
      "Loss for epoch #9: 3762648.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.5790848135948181\n",
      "\n",
      "Starting training model 21. Will train for 2 epochs.\n",
      "model_type: skipgram, window_size: 7, embedding_size: 100\n",
      "Loss for epoch #0: 15892559.0\n",
      "Loss for epoch #1: 12156553.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.689628541469574\n",
      "\n",
      "Starting training model 22. Will train for 5 epochs.\n",
      "model_type: skipgram, window_size: 7, embedding_size: 100\n",
      "Loss for epoch #0: 15558958.0\n",
      "Loss for epoch #1: 11521664.0\n",
      "Loss for epoch #2: 9282630.0\n",
      "Loss for epoch #3: 6761452.0\n",
      "Loss for epoch #4: 6756132.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.6227489113807678\n",
      "\n",
      "Starting training model 23. Will train for 10 epochs.\n",
      "model_type: skipgram, window_size: 7, embedding_size: 100\n",
      "Loss for epoch #0: 16311252.0\n",
      "Loss for epoch #1: 11374022.0\n",
      "Loss for epoch #2: 9189822.0\n",
      "Loss for epoch #3: 6576364.0\n",
      "Loss for epoch #4: 6626964.0\n",
      "Loss for epoch #5: 6388436.0\n",
      "Loss for epoch #6: 6269824.0\n",
      "Loss for epoch #7: 4481476.0\n",
      "Loss for epoch #8: 324360.0\n",
      "Loss for epoch #9: 319848.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.583570122718811\n",
      "\n",
      "Starting training model 24. Will train for 2 epochs.\n",
      "model_type: skipgram, window_size: 15, embedding_size: 100\n",
      "Loss for epoch #0: 25813020.0\n",
      "Loss for epoch #1: 17392568.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.665330708026886\n",
      "\n",
      "Starting training model 25. Will train for 5 epochs.\n",
      "model_type: skipgram, window_size: 15, embedding_size: 100\n",
      "Loss for epoch #0: 24935712.0\n",
      "Loss for epoch #1: 16679348.0\n",
      "Loss for epoch #2: 13012644.0\n",
      "Loss for epoch #3: 12489536.0\n",
      "Loss for epoch #4: 557912.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.6101263165473938\n",
      "\n",
      "Starting training model 26. Will train for 10 epochs.\n",
      "model_type: skipgram, window_size: 15, embedding_size: 100\n",
      "Loss for epoch #0: 24647734.0\n",
      "Loss for epoch #1: 16337002.0\n",
      "Loss for epoch #2: 12875916.0\n",
      "Loss for epoch #3: 12467708.0\n",
      "Loss for epoch #4: 1726480.0\n",
      "Loss for epoch #5: 936176.0\n",
      "Loss for epoch #6: 846104.0\n",
      "Loss for epoch #7: 803952.0\n",
      "Loss for epoch #8: 706336.0\n",
      "Loss for epoch #9: 668568.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.5763540267944336\n",
      "\n",
      "Starting training model 27. Will train for 2 epochs.\n",
      "model_type: cbow, window_size: 3, embedding_size: 5\n",
      "Loss for epoch #0: 9535974.0\n",
      "Loss for epoch #1: 7283952.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.7374587655067444\n",
      "\n",
      "Starting training model 28. Will train for 5 epochs.\n",
      "model_type: cbow, window_size: 3, embedding_size: 5\n",
      "Loss for epoch #0: 9560769.0\n",
      "Loss for epoch #1: 7189186.0\n",
      "Loss for epoch #2: 6090295.0\n",
      "Loss for epoch #3: 6084482.0\n",
      "Loss for epoch #4: 5358944.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.7469581961631775\n",
      "\n",
      "Starting training model 29. Will train for 10 epochs.\n",
      "model_type: cbow, window_size: 3, embedding_size: 5\n",
      "Loss for epoch #0: 9545247.0\n",
      "Loss for epoch #1: 7255591.0\n",
      "Loss for epoch #2: 6010302.0\n",
      "Loss for epoch #3: 6053760.0\n",
      "Loss for epoch #4: 5367376.0\n",
      "Loss for epoch #5: 2995604.0\n",
      "Loss for epoch #6: 2888340.0\n",
      "Loss for epoch #7: 2833620.0\n",
      "Loss for epoch #8: 2768336.0\n",
      "Loss for epoch #9: 2729004.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.7452654242515564\n",
      "\n",
      "Starting training model 30. Will train for 2 epochs.\n",
      "model_type: cbow, window_size: 7, embedding_size: 5\n",
      "Loss for epoch #0: 8943320.0\n",
      "Loss for epoch #1: 6746717.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.6681346297264099\n",
      "\n",
      "Starting training model 31. Will train for 5 epochs.\n",
      "model_type: cbow, window_size: 7, embedding_size: 5\n",
      "Loss for epoch #0: 8991708.0\n",
      "Loss for epoch #1: 6963765.0\n",
      "Loss for epoch #2: 6066077.0\n",
      "Loss for epoch #3: 5970308.0\n",
      "Loss for epoch #4: 5749566.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.6783523559570312\n",
      "\n",
      "Starting training model 32. Will train for 10 epochs.\n",
      "model_type: cbow, window_size: 7, embedding_size: 5\n",
      "Loss for epoch #0: 8950443.0\n",
      "Loss for epoch #1: 6894328.0\n",
      "Loss for epoch #2: 6019045.0\n",
      "Loss for epoch #3: 5907906.0\n",
      "Loss for epoch #4: 5819642.0\n",
      "Loss for epoch #5: 2851016.0\n",
      "Loss for epoch #6: 2732392.0\n",
      "Loss for epoch #7: 2671520.0\n",
      "Loss for epoch #8: 2603752.0\n",
      "Loss for epoch #9: 2496240.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.6903070211410522\n",
      "\n",
      "Starting training model 33. Will train for 2 epochs.\n",
      "model_type: cbow, window_size: 15, embedding_size: 5\n",
      "Loss for epoch #0: 7441841.5\n",
      "Loss for epoch #1: 5791955.5\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.6210688352584839\n",
      "\n",
      "Starting training model 34. Will train for 5 epochs.\n",
      "model_type: cbow, window_size: 15, embedding_size: 5\n",
      "Loss for epoch #0: 7387061.0\n",
      "Loss for epoch #1: 5812541.0\n",
      "Loss for epoch #2: 5327616.0\n",
      "Loss for epoch #3: 4818828.0\n",
      "Loss for epoch #4: 4873582.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.638331413269043\n",
      "\n",
      "Starting training model 35. Will train for 10 epochs.\n",
      "model_type: cbow, window_size: 15, embedding_size: 5\n",
      "Loss for epoch #0: 7230434.5\n",
      "Loss for epoch #1: 5719675.5\n",
      "Loss for epoch #2: 5243564.0\n",
      "Loss for epoch #3: 4727364.0\n",
      "Loss for epoch #4: 4761578.0\n",
      "Loss for epoch #5: 4758300.0\n",
      "Loss for epoch #6: 2785220.0\n",
      "Loss for epoch #7: 2112784.0\n",
      "Loss for epoch #8: 2041292.0\n",
      "Loss for epoch #9: 1937000.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.6558539271354675\n",
      "\n",
      "Starting training model 36. Will train for 2 epochs.\n",
      "model_type: cbow, window_size: 3, embedding_size: 25\n",
      "Loss for epoch #0: 6687430.0\n",
      "Loss for epoch #1: 4897956.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.6032377481460571\n",
      "\n",
      "Starting training model 37. Will train for 5 epochs.\n",
      "model_type: cbow, window_size: 3, embedding_size: 25\n",
      "Loss for epoch #0: 6653388.0\n",
      "Loss for epoch #1: 4810942.0\n",
      "Loss for epoch #2: 4525958.0\n",
      "Loss for epoch #3: 3808038.0\n",
      "Loss for epoch #4: 3668486.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.5950146913528442\n",
      "\n",
      "Starting training model 38. Will train for 10 epochs.\n",
      "model_type: cbow, window_size: 3, embedding_size: 25\n",
      "Loss for epoch #0: 6616171.5\n",
      "Loss for epoch #1: 4915730.5\n",
      "Loss for epoch #2: 4523989.0\n",
      "Loss for epoch #3: 3845901.0\n",
      "Loss for epoch #4: 3691214.0\n",
      "Loss for epoch #5: 3667558.0\n",
      "Loss for epoch #6: 3702368.0\n",
      "Loss for epoch #7: 3140192.0\n",
      "Loss for epoch #8: 1858164.0\n",
      "Loss for epoch #9: 1789220.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.599748969078064\n",
      "\n",
      "Starting training model 39. Will train for 2 epochs.\n",
      "model_type: cbow, window_size: 7, embedding_size: 25\n",
      "Loss for epoch #0: 5794173.5\n",
      "Loss for epoch #1: 4551009.5\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.551035463809967\n",
      "\n",
      "Starting training model 40. Will train for 5 epochs.\n",
      "model_type: cbow, window_size: 7, embedding_size: 25\n",
      "Loss for epoch #0: 5877807.0\n",
      "Loss for epoch #1: 4536977.0\n",
      "Loss for epoch #2: 4104053.0\n",
      "Loss for epoch #3: 3752471.0\n",
      "Loss for epoch #4: 3328422.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.547508180141449\n",
      "\n",
      "Starting training model 41. Will train for 10 epochs.\n",
      "model_type: cbow, window_size: 7, embedding_size: 25\n",
      "Loss for epoch #0: 5841551.5\n",
      "Loss for epoch #1: 4521795.5\n",
      "Loss for epoch #2: 4098949.0\n",
      "Loss for epoch #3: 3797516.0\n",
      "Loss for epoch #4: 3458626.0\n",
      "Loss for epoch #5: 3353962.0\n",
      "Loss for epoch #6: 3326208.0\n",
      "Loss for epoch #7: 3317096.0\n",
      "Loss for epoch #8: 2542936.0\n",
      "Loss for epoch #9: 1507196.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.5671772956848145\n",
      "\n",
      "Starting training model 42. Will train for 2 epochs.\n",
      "model_type: cbow, window_size: 15, embedding_size: 25\n",
      "Loss for epoch #0: 4703394.5\n",
      "Loss for epoch #1: 3858825.5\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.5151773691177368\n",
      "\n",
      "Starting training model 43. Will train for 5 epochs.\n",
      "model_type: cbow, window_size: 15, embedding_size: 25\n",
      "Loss for epoch #0: 4709345.5\n",
      "Loss for epoch #1: 3794493.5\n",
      "Loss for epoch #2: 3307886.0\n",
      "Loss for epoch #3: 3281622.0\n",
      "Loss for epoch #4: 2980209.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.5184527039527893\n",
      "\n",
      "Starting training model 44. Will train for 10 epochs.\n",
      "model_type: cbow, window_size: 15, embedding_size: 25\n",
      "Loss for epoch #0: 4737421.5\n",
      "Loss for epoch #1: 3847887.5\n",
      "Loss for epoch #2: 3370287.0\n",
      "Loss for epoch #3: 3325754.0\n",
      "Loss for epoch #4: 2975132.0\n",
      "Loss for epoch #5: 2680724.0\n",
      "Loss for epoch #6: 2619348.0\n",
      "Loss for epoch #7: 2621736.0\n",
      "Loss for epoch #8: 2607104.0\n",
      "Loss for epoch #9: 2566958.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.5347798466682434\n",
      "\n",
      "Starting training model 45. Will train for 2 epochs.\n",
      "model_type: cbow, window_size: 3, embedding_size: 100\n",
      "Loss for epoch #0: 6909935.5\n",
      "Loss for epoch #1: 4799638.5\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.545290470123291\n",
      "\n",
      "Starting training model 46. Will train for 5 epochs.\n",
      "model_type: cbow, window_size: 3, embedding_size: 100\n",
      "Loss for epoch #0: 6780751.0\n",
      "Loss for epoch #1: 4796994.0\n",
      "Loss for epoch #2: 4359156.0\n",
      "Loss for epoch #3: 3663979.0\n",
      "Loss for epoch #4: 3468478.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.49814364314079285\n",
      "\n",
      "Starting training model 47. Will train for 10 epochs.\n",
      "model_type: cbow, window_size: 3, embedding_size: 100\n",
      "Loss for epoch #0: 6779752.5\n",
      "Loss for epoch #1: 4749234.5\n",
      "Loss for epoch #2: 4344624.0\n",
      "Loss for epoch #3: 3679505.0\n",
      "Loss for epoch #4: 3491714.0\n",
      "Loss for epoch #5: 3404264.0\n",
      "Loss for epoch #6: 3360784.0\n",
      "Loss for epoch #7: 3259284.0\n",
      "Loss for epoch #8: 1983890.0\n",
      "Loss for epoch #9: 1728088.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.4756515920162201\n",
      "\n",
      "Starting training model 48. Will train for 2 epochs.\n",
      "model_type: cbow, window_size: 7, embedding_size: 100\n",
      "Loss for epoch #0: 6297615.5\n",
      "Loss for epoch #1: 4633060.5\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.5019983649253845\n",
      "\n",
      "Starting training model 49. Will train for 5 epochs.\n",
      "model_type: cbow, window_size: 7, embedding_size: 100\n",
      "Loss for epoch #0: 6397244.0\n",
      "Loss for epoch #1: 4546164.0\n",
      "Loss for epoch #2: 4161248.0\n",
      "Loss for epoch #3: 3662904.0\n",
      "Loss for epoch #4: 3309136.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.46797820925712585\n",
      "\n",
      "Starting training model 50. Will train for 10 epochs.\n",
      "model_type: cbow, window_size: 7, embedding_size: 100\n",
      "Loss for epoch #0: 6243120.0\n",
      "Loss for epoch #1: 4634403.0\n",
      "Loss for epoch #2: 4266240.0\n",
      "Loss for epoch #3: 3734315.0\n",
      "Loss for epoch #4: 3403930.0\n",
      "Loss for epoch #5: 3306158.0\n",
      "Loss for epoch #6: 3246738.0\n",
      "Loss for epoch #7: 3197846.0\n",
      "Loss for epoch #8: 2408246.0\n",
      "Loss for epoch #9: 1595980.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.45858049392700195\n",
      "\n",
      "Starting training model 51. Will train for 2 epochs.\n",
      "model_type: cbow, window_size: 15, embedding_size: 100\n",
      "Loss for epoch #0: 5337987.0\n",
      "Loss for epoch #1: 4168092.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.4712039828300476\n",
      "\n",
      "Starting training model 52. Will train for 5 epochs.\n",
      "model_type: cbow, window_size: 15, embedding_size: 100\n",
      "Loss for epoch #0: 5320548.5\n",
      "Loss for epoch #1: 4084689.5\n",
      "Loss for epoch #2: 3575029.0\n",
      "Loss for epoch #3: 3443122.0\n",
      "Loss for epoch #4: 2909103.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.4433695673942566\n",
      "\n",
      "Starting training model 53. Will train for 10 epochs.\n",
      "model_type: cbow, window_size: 15, embedding_size: 100\n",
      "Loss for epoch #0: 5355494.0\n",
      "Loss for epoch #1: 4149823.0\n",
      "Loss for epoch #2: 3652671.0\n",
      "Loss for epoch #3: 3589879.0\n",
      "Loss for epoch #4: 2916707.0\n",
      "Loss for epoch #5: 2856636.0\n",
      "Loss for epoch #6: 2825668.0\n",
      "Loss for epoch #7: 2808584.0\n",
      "Loss for epoch #8: 2718482.0\n",
      "Loss for epoch #9: 2671554.0\n",
      "[WARNING] A total of 438 samples were ignored because they contained words out of the model's vocabulary.\n",
      "Final analogies score: 0.437330424785614\n",
      "\n",
      "CPU times: user 14h 21min 22s, sys: 6min 5s, total: 14h 27min 27s\n",
      "Wall time: 2h 11min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    \"model_type\": [\"skipgram\", \"cbow\"],\n",
    "    \"embedding_size\": [5, 25, 100],\n",
    "    \"window_size\": [3, 7, 15],\n",
    "    \"epochs\": [2, 5, 10],\n",
    "}\n",
    "\n",
    "grid_search_results = run_grid_search(\n",
    "    train_dataset=train_dataset,\n",
    "    param_grid=param_grid,\n",
    "    test_analogies=all_test_analogies,\n",
    "    return_best=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_params = max(grid_search_results, key=lambda x: x[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_model_params = min(grid_search_results, key=lambda x: x[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.43733042, 0.91239774)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_model_params[\"score\"], best_model_params[\"score\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
